{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Fashion Model-4.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "tAY6O56_sEHD",
        "colab_type": "code",
        "outputId": "392dc35a-aa96-4053-9cbd-e797a3121569",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "#from __future__ import print_function\n",
        "import keras\n",
        "from keras.datasets import fashion_mnist\n",
        "from keras.layers import Dense, Conv2D, BatchNormalization, Activation\n",
        "from keras.layers import AveragePooling2D, Input, Flatten\n",
        "from keras.optimizers import Adam\n",
        "from keras.callbacks import TensorBoard\n",
        "import matplotlib.pyplot as plt\n",
        "from keras.utils import to_categorical\n",
        "from keras.optimizers import Adam\n",
        "from keras.callbacks import ModelCheckpoint, LearningRateScheduler\n",
        "from keras.callbacks import ReduceLROnPlateau\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.regularizers import l2\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras import backend as K\n",
        "from keras.models import Model\n",
        "from keras.models import Sequential\n",
        "\n",
        "from keras.layers import Conv2D, Dense, Flatten, MaxPooling2D, Dropout\n",
        "from keras.callbacks import LearningRateScheduler, TensorBoard\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.regularizers import l2\n",
        "from keras.datasets import cifar10\n",
        "import numpy as np\n",
        "import os, time"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gUYmHth5sSm1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Training parameters\n",
        "batch_size = 32  # orig paper trained all networks with batch_size=128\n",
        "epochs = 10\n",
        "height= 28\n",
        "width=28\n",
        "data_augmentation = False\n",
        "num_classes = 10\n",
        "\n",
        "# Subtracting pixel mean improves accuracy\n",
        "subtract_pixel_mean = True"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lMMsJCAVsFtc",
        "colab_type": "code",
        "outputId": "010f56ae-267e-4c44-cb98-025dafad61a6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "(x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()\n",
        "\n",
        "# Normalize data.\n",
        "x_train = x_train.astype('float32') / 255\n",
        "x_test = x_test.astype('float32') / 255\n",
        "    \n",
        "# #split into train and validation\n",
        "# (x_train, x_val, y_train, y_val) = train_test_split(x_train, y_train, test_size = 0.15, random_state = 0)\n",
        "# print(\"shapes after splitting \", x_train.shape, y_train.shape, x_val.shape, y_val.shape, x_test.shape, y_test.shape)\n",
        "    \n",
        "#one hot encoding\n",
        "y_train= to_categorical(y_train, num_classes) \n",
        "# y_val= to_categorical(y_val, num_classes) \n",
        "y_test= to_categorical(y_test, num_classes)\n",
        "\n",
        "print(\"after hot encoding\", y_train.shape, y_test.shape)\n",
        "    \n",
        "#reshape data\n",
        "x_train = x_train.reshape(x_train.shape[0], height, width, 1)\n",
        "x_test = x_test.reshape(x_test.shape[0], height, width, 1)\n",
        "# x_val = x_val.reshape(x_val.shape[0], height, width, 1)\n",
        "\n",
        "# If subtract pixel mean is enabled\n",
        "if subtract_pixel_mean:\n",
        "    x_train_mean = np.mean(x_train, axis=0)\n",
        "    x_train -= x_train_mean\n",
        "    x_test -= x_train_mean"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "after hot encoding (60000, 10) (10000, 10)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3wADHjFksfmy",
        "colab_type": "code",
        "outputId": "b1e87ef7-8389-411d-e10d-a48616815c23",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 328
        }
      },
      "source": [
        "model = Sequential()\n",
        "model.add(Conv2D(16, (3, 3), padding='same', activation = 'relu', kernel_initializer='he_normal',  input_shape=(28,28,1) ) )\n",
        "model.add(Dropout(0.15))\n",
        "model.add(Conv2D(32, (3, 3), padding='same', activation = 'relu', kernel_initializer='he_normal' )) \n",
        "model.add(Dropout(0.25))\n",
        "model.add(Conv2D(64, (3, 3), padding='same', activation = 'relu', kernel_initializer='he_normal' )) \n",
        "model.add(Dropout(0.25))\n",
        "model.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
        "\n",
        "model.add(Flatten())\n",
        "    \n",
        "model.add(Dense(128, activation = 'relu'))\n",
        "model.add(Dropout(0.25))\n",
        "model.add(Dense(64, activation = 'relu'))\n",
        "model.add(Dropout(0.10))\n",
        "\n",
        "model.add(Dense(10, activation = 'softmax'))\n",
        "   "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0704 07:04:59.900972 140192682633088 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "W0704 07:04:59.918806 140192682633088 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "W0704 07:04:59.922251 140192682633088 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4185: The name tf.truncated_normal is deprecated. Please use tf.random.truncated_normal instead.\n",
            "\n",
            "W0704 07:04:59.943669 140192682633088 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:133: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
            "\n",
            "W0704 07:04:59.955943 140192682633088 deprecation.py:506] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "W0704 07:05:00.037448 140192682633088 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3976: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
            "\n",
            "W0704 07:05:00.051562 140192682633088 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cru4ZLXIziID",
        "colab_type": "code",
        "outputId": "fa2b6c3b-1cbf-4efa-ee63-4c25eda305e6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        }
      },
      "source": [
        "model.compile(loss='categorical_crossentropy', optimizer=Adam(), metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "W0704 07:06:02.773678 140192682633088 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zxgXDIcct4tC",
        "colab_type": "code",
        "outputId": "2bd189ac-c973-4ba9-942c-d85b376c2167",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 492
        }
      },
      "source": [
        "# Run training, with or without data augmentation.\n",
        "if not data_augmentation:\n",
        "    print('Not using data augmentation.')\n",
        "    start_time= time.time()\n",
        "    history= model.fit(x_train, y_train,\n",
        "              batch_size=batch_size,\n",
        "              epochs=epochs,\n",
        "              shuffle=True,\n",
        "              )\n",
        "    end_time= time.time()\n",
        "    print(\"testing time\", (end_time-start_time))\n",
        "    \n",
        "else:\n",
        "    print('Using real-time data augmentation.')\n",
        "    # This will do preprocessing and realtime data augmentation:\n",
        "    datagen = ImageDataGenerator(\n",
        "        # set input mean to 0 over the dataset\n",
        "        featurewise_center=False,\n",
        "        # set each sample mean to 0\n",
        "        samplewise_center=False,\n",
        "        # divide inputs by std of dataset\n",
        "        featurewise_std_normalization=False,\n",
        "        # divide each input by its std\n",
        "        samplewise_std_normalization=False,\n",
        "        # apply ZCA whitening\n",
        "        zca_whitening=False,\n",
        "        # epsilon for ZCA whitening\n",
        "        zca_epsilon=1e-06,\n",
        "        # randomly rotate images in the range (deg 0 to 180)\n",
        "        rotation_range=0,\n",
        "        # randomly shift images horizontally\n",
        "        width_shift_range=0.1,\n",
        "        # randomly shift images vertically\n",
        "        height_shift_range=0.1,\n",
        "        # set range for random shear\n",
        "        shear_range=0.,\n",
        "        # set range for random zoom\n",
        "        zoom_range=0.,\n",
        "        # set range for random channel shifts\n",
        "        channel_shift_range=0.,\n",
        "        # set mode for filling points outside the input boundaries\n",
        "        fill_mode='nearest',\n",
        "        # value used for fill_mode = \"constant\"\n",
        "        cval=0.,\n",
        "        # randomly flip images\n",
        "        horizontal_flip=True,\n",
        "        # randomly flip images\n",
        "        vertical_flip=False,\n",
        "        # set rescaling factor (applied before any other transformation)\n",
        "        rescale=None,\n",
        "        # set function that will be applied on each input\n",
        "        preprocessing_function=None,\n",
        "        # image data format, either \"channels_first\" or \"channels_last\"\n",
        "        data_format=None,\n",
        "        # fraction of images reserved for validation (strictly between 0 and 1)\n",
        "        validation_split=0.0)\n",
        "\n",
        "    # Compute quantities required for featurewise normalization\n",
        "    # (std, mean, and principal components if ZCA whitening is applied).\n",
        "    datagen.fit(x_train)\n",
        "\n",
        "    # Fit the model on the batches generated by datagen.flow().\n",
        "    start_time= time.time()\n",
        "    history=model.fit_generator(datagen.flow(x_train, y_train, batch_size=batch_size),\n",
        "                \n",
        "                        epochs=epochs, verbose=1, workers=4,\n",
        "                        )\n",
        "    end_time= time.time()\n",
        "    print(\"testing time\", (end_time-start_time))\n",
        "   "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "W0704 07:06:06.657441 140192682633088 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Not using data augmentation.\n",
            "Epoch 1/10\n",
            "60000/60000 [==============================] - 348s 6ms/step - loss: 0.5006 - acc: 0.8219\n",
            "Epoch 2/10\n",
            "60000/60000 [==============================] - 347s 6ms/step - loss: 0.3165 - acc: 0.8880\n",
            "Epoch 3/10\n",
            "60000/60000 [==============================] - 347s 6ms/step - loss: 0.2694 - acc: 0.9036\n",
            "Epoch 4/10\n",
            "60000/60000 [==============================] - 348s 6ms/step - loss: 0.2382 - acc: 0.9129\n",
            "Epoch 5/10\n",
            "60000/60000 [==============================] - 365s 6ms/step - loss: 0.2148 - acc: 0.9210\n",
            "Epoch 6/10\n",
            "60000/60000 [==============================] - 354s 6ms/step - loss: 0.2022 - acc: 0.9261\n",
            "Epoch 7/10\n",
            "60000/60000 [==============================] - 352s 6ms/step - loss: 0.1867 - acc: 0.9313\n",
            "Epoch 8/10\n",
            "60000/60000 [==============================] - 351s 6ms/step - loss: 0.1747 - acc: 0.9365\n",
            "Epoch 9/10\n",
            "60000/60000 [==============================] - 353s 6ms/step - loss: 0.1635 - acc: 0.9394\n",
            "Epoch 10/10\n",
            "60000/60000 [==============================] - 358s 6ms/step - loss: 0.1528 - acc: 0.9436\n",
            "testing time 3522.593101978302\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ztz9-lxzAon6",
        "colab_type": "code",
        "outputId": "c13656e4-28e5-402b-d0ba-0b56fdff7f01",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        }
      },
      "source": [
        "# Score trained model.\n",
        "start_time= time.time()\n",
        "scores = model.evaluate(x_test, y_test, verbose=1)\n",
        "end_time= time.time()\n",
        "print(\"testing time\", (end_time-start_time))\n",
        "print('Test loss:', scores[0])\n",
        "print('Test accuracy:', scores[1])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "10000/10000 [==============================] - 10s 1ms/step\n",
            "testing time 10.492141008377075\n",
            "Test loss: 0.2255352610707283\n",
            "Test accuracy: 0.9216\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_f2VIkN-zmKI",
        "colab_type": "code",
        "outputId": "7a074c72-f39a-45c9-a5aa-b8290de5d1c6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 244
        }
      },
      "source": [
        "#Training history visualization\n",
        "\n",
        "# Plot training & validation accuracy values\n",
        "plt.plot(history.history['acc'])\n",
        "plt.plot(history.history['val_acc'])\n",
        "plt.title('Model accuracy')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Test'], loc='upper left')\n",
        "plt.show()\n",
        "\n",
        "# Plot training & validation loss values\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('Model loss')\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Test'], loc='upper left')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-7759714798e8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'acc'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'val_acc'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Model accuracy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mylabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Accuracy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxlabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Epoch'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'history' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SKyO2xBGEwJP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}